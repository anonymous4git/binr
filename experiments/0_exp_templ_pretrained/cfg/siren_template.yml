root: "<ROOT>"
data: "<ROOT>/templ_48dir.nii.gz"
mask: "<ROOT>/MNI152_T1_2mm_brain_mask_dil.nii.gz"
bvec: "<ROOT>/templ_bvecs"
bval: "<ROOT>/templ_bvals"

# model training
train_batch_size: 262_144 # 256, prev.4096 (?)
epochs: 64

# model training hyperparameters
lr: 0.001
use_cos_lrsheduler: True
# model config
in_size: 301 # 256 + 45 (lmax=8, antipodal symmetric) ,7 for [px, py, pz, b, qx, qy, qz]
hidden_size: 256
out_size: 1
num_layers: 3

# validation settings
val_mode: "epoch" # either 'step' or 'epoch'
val_skip: True
val_batch_size: 25230
val_slice_axis: 2
val_slice_offset: 45
val_check_interval: 500 # (currently:never called!) prop. need less often
val_sanity_steps: 0
val_b_idx: [0, 48, 96, 144]
val_export_dwi: False

bval_train: "all"
bval_val: "all"

# tb logging
log_coords_raw: False
log_coords_encoded: False
steps_per_viz: 500 # Visualization frequency in training steps

# checkpointing
checkpoint_dirpath: "OUTPUTS/checkpoints_siren_templates_new"
checkpoint_filename: "inr-{step}"
checkpoint_save_top_k: -1 # Keep all checkpoints
checkpoint_every_n_train_steps: 1000
# checkpoint_path: null  # Path to load checkpoint from (uncomment and set to load from checkpoint)
evaluation_checkpoint: "OUTPUTS/checkpoints_siren_templates_new/last.ckpt"
network_name: "siren_mlp"
pos_encoder: True
normalize_0_1: True
