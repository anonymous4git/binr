root: "<ROOT>/<SUBJECT>/Diffusion_MNIWarped"
data: "<ROOT>/<SUBJECT>/Diffusion_MNIWarped/dwi_MNI152.nii.gz"
mask: "<ROOT>/MNI152_T1_2mm_brain_mask_dil.nii.gz"
bvec: "<ROOT>/<SUBJECT>/Diffusion_MNIWarped/bvecs.sorted"
bval: "<ROOT>/<SUBJECT>/Diffusion_MNIWarped/bvals.sorted"

# training optimization
preload_to_gpu: True
train_log_interval: 1 # log every ... batches only for better gpu usage

# model training
train_batch_size: 262_144 # 256, prev.4096 (?)
epochs: 16 # 19

# model training hyperparameters
lr: 0.0001
use_cos_lrsheduler: True

# model config
in_size: 301 # 256 + 45 (lmax=8, antipodal symmetric) ,7 for [px, py, pz, b, qx, qy, qz]
hidden_size: 256
out_size: 1
num_layers: 3

# validation settings
val_mode: "epoch" # either 'step' or 'epoch'
val_skip: True
val_batch_size: 25230
val_slice_axis: 2
val_slice_offset: 45
val_check_interval: 999 # prop. need less often
val_sanity_steps: 0
val_b_idx: [0, 48, 96, 144]
val_export_dwi: False

bval_train: "all"
bval_val: "all"

# tb logging
log_coords_raw: False
log_coords_encoded: False
steps_per_viz: 500 # Visualization frequency in training steps

# checkpointing
checkpoint_dirpath: "checkpoints_warped/<SUBJECT>"
checkpoint_filename: "wiren_<SUBJECT>-{step}"
checkpoint_save_top_k: -1 # Keep all checkpoints
checkpoint_every_n_train_steps: 100
checkpoint_path: "OUTPUTS/checkpoints_wiren_templates_new/last.ckpt":wq

evaluation_checkpoint: "checkpoints_warped/<SUBJECT>/last.ckpt"
network_name: "wiren_mlp"
pos_encoder: True
normalize_0_1: True
